---
title: 'Homework 4'
header-includes:
  - \usepackage{multirow}
output:
  pdf_document: default
urlcolor: blue
---

```{r, include=FALSE}

library(tidyverse)
library(here)
library(corrplot)
library(tidyr)
library(knitr)
library(dplyr)
library(ggplot2)
library(dplyr)
library(cowplot)
library(kableExtra)
library(patchwork)
library(broom)
library(pheatmap)
library(quantreg)
library(purrr)
library(LowRankQP)
library(glmnet)

knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.width = 16,
  fig.height = 10
)

set.seed(2025)
```

## Context

This assignment reinforces ideas in Module 4: Constrained Optimization. We focus specifically on implementing quantile regression and LASSO.


## Due date and submission

Please submit (via Canvas) a PDF containing a link to the web address of the GitHub repo containing your work for this assignment; git commits after the due date will cause the assignment to be considered late. Due date is Wednesday, 4/2 at 10:00AM.

Repository link: https://github.com/dliao1/bios731_hw4_liao


## Points

```{r, echo = FALSE}
tibble(
  Problem = c("Problem 0", "Problem 1", "Problem 2", "Problem 3"),
  Points = c(20, 20, 30, 30)
) %>%
  knitr::kable()
```

## Dataset 

The dataset for this homework assignment is in the file `cannabis.rds`. It comes from a study conducted by researchers at the University of Colorado who are working to develop roadside tests for detecting driving impairment due to cannabis use.  In this study, researchers measured levels of THC—the main psychoactive ingredient in cannabis—in participants’ blood and then collected other biomarkers and had them complete a series of neurocognitive tests. The goal of the study is to understand the relationship between performance on these neurocognitive tests and the concentration of THC metabolites in the blood. 

The dataset contains the following variables:

* `id`: subject id
* `t_mmr1`: Metabolite molar ratio—a measure of THC metabolites in the blood. This is the outcome variable.
* `p_*`: variables with the `p_` prefix contain measurements related to pupil response to light. 
* `i_*`: variables with the `i_` prefix were collected using an iPad and are derived from neurocognitive tests assessing reaction time, judgment, and short-term memory.
* `h_*`: Variables related to heart rate and blood pressure.



## Problem 0 

This "problem" focuses on structure of your submission, especially the use git and GitHub for reproducibility, R Projects to organize your work, R Markdown to write reproducible reports, relative paths to load data from local files, and reasonable naming structures for your files.

To that end:

* Create a public GitHub repo + local R Project; I suggest naming this repo / directory bios731_hw4_YourLastName (e.g. bios731_hw4_wrobel for Julia)
* Submit your whole project folder to GitHub 
* Submit a PDF knitted from Rmd to Canvas. Your solutions to the problems here should be implemented in your .Rmd file, and your git commit history should reflect the process you used to solve these Problems.

## Problem 1: Exploratory data analysis

Perform some EDA for this data. Your EDA should explore the following questions:

- What are $n$ and $p$ for this data?
- What is the distribution of the outcome?
- How correlated are variables in the dataset?

Summarize key findings from your EDA in one paragraph and 2-3 figures or tables. 

```{r}
cannabis_df <- readRDS(here("data", "cannabis-2.rds"))
```

```{r}
cor_matrix <- cor(cannabis_df %>%
                    select(-c(id)),
                  use = "complete.obs")
corrplot(cor_matrix)

cor_high <- as_tibble(which(upper.tri(cor_matrix) & (abs(cor_matrix) > 0.5), arr.ind = TRUE))

cor_high_df <- tibble(
  var1 = rownames(cor_matrix)[cor_high$row],
  var2 = colnames(cor_matrix)[cor_high$col],
  correlation = cor_matrix[cbind(cor_high$row, cor_high$col)]
)

kable(cor_high_df)
```

```{r}
nrow(cannabis_df)
ncol(cannabis_df) - 2 # predictors = number of columns - 1 (id col) - 1 (outcome)
```

```{r}
hist(cannabis_df$t_mmr1)

t_mmr1_median <- quantile(cannabis_df$t_mmr1)[[3]]

ggplot(cannabis_df, aes(x = h_hr, y = t_mmr1, 
                        color = ifelse(t_mmr1 > t_mmr1_median, "green", "red"))) +
  geom_point(size = 3) +
  scale_color_identity() +
  labs(title = "h_hr Colored Based on t_mmr1",
       x = "h_hr",
       y = "t_mmr1") +
  theme_minimal(base_size = 16)


ggplot(cannabis_df, aes(x = p_change, y = t_mmr1, 
                        color = ifelse(t_mmr1 > t_mmr1_median, "green", "red"))) +
  geom_point(size = 3) +
  scale_color_identity() +
  labs(title = "p_change Colored Based on t_mmr1",
       x = "p_change",
       y = "t_mmr1") +
  theme_minimal(base_size = 16)

```

### Summary 

To summarize, n is 57 for the 57 individuals in the dataset and p is 27 for 
the number of predictors (covariates in the dataset). After
creating a separate table of variables that had correlations higher than
an absolute value of 0.5, it looks like most of the variables that had to do with
the iPad neurocognitive tests were pretty strongly correlated - for example, i_rep_shapes34
and i_composite_score had a strong negative correlation of -0.9 but i_memory_time34 
and i_composite_score had a moderately positive correlation of 0.6. 

The distribution of the outcome is highly right skewed - looking at the histogram, it looks like the 
majority of values are clustered by a metabolite molar ratio of THC of 0. This is
partly why I chose to include the two graphs plotting covariates like
heart rate and p_change and colored points based on whether the 
molar ratio of THC was 0 or not - there aren't many obvious linear trends,
but it looks like the individuals with higher THC molar ratios 
tended to have a larger spread of both both heart rates and percentage
changes in pupil responses.



## Problem 2: Quantile regression

Use linear programming to estimate the coefficients for a quantile regression. You need to write a
function named `my_rq`, which takes a response vector $y$, a covariate matrix $X$ and quantile $\tau$ , and
returns the estimated coefficients. Existing linear programming functions can be used directly to
solve the LP problem (for example, `simplex` function in the `boot` package, or `lp` function in the `lpSolve`
package). 

* Use your function to model `t_mmr1` from the cannabis data using `p_change` (percent change in pupil diameter in response to light), `h_hr` (heart rate), and `i_composite_score` (a composite score of the ipad variables) as variables.
* Compare your results with though estimated using the `rq` function in R at quantiles $\tau \in \{0.25, 0.5, 0.75\}$.
* Compare with mean obtain using linear regression
* Summarize findings


When explaining your results, be sure to explain what LP method you used for estimating quantile regression.

### My Implementation

```{r}
library(boot)
my_rq <- function(y, X, tau) {
  n <- length(y)
  p <- ncol(X)
  
  # See slide 16 of class notes
  # u <- [yi - xi*beta]+
  # v <- [yi - xi*beta]-
  # We need 2*p instead of just p because we're splitting coefficients into
  # positive and negative
  
  #0*b+         #0*b-    # t * ui      #(1 - t)vi
  obj <- -1 * c(rep(0, p), rep(0, p), rep(tau, n), rep(1 - tau, n))
  
  # Constraints:
  # y_i = X_i b+ - X_i b- + u_i - v_i
  A <- cbind(X, -X, diag(n), -diag(n))
  b <- y
  
  # Using A3 for equality here
  result <- simplex(a = obj, A3 = A, b3 = b, maxi = TRUE)
  
  # Remember b+ is 1:p and b- is p+1:2p and y_i = X_i b+ - X_i b- + u_i - v_i
  solution <- result$soln
  
  b_plus <- solution[1:p]
  b_minus <- solution[(p + 1):(2 * p)]
  
  # Get final coefficients: beta = b+ - b- for xi
  beta_hat <- b_plus - b_minus
  
  beta_df <- as_tibble(as.list(beta_hat)) %>% 
    mutate(tau = tau) %>%
    rename(intercept = x1) %>%
    select(tau, everything())
  
  
  return (beta_df)
}


y <- cannabis_df$t_mmr1
X <- cbind(1, cannabis_df$p_change, cannabis_df$h_hr, cannabis_df$i_composite_score)
quantiles <- c(0.25, 0.5, 0.75)

quantile_results <- map_dfr(quantiles, ~ my_rq(y, X, .x), progress = TRUE)

kable(quantile_results)
```

### Using rq() in R
```{r}
rq_results <- tidy(rq(t_mmr1 ~ p_change + h_hr + i_composite_score, tau = quantiles, data = cannabis_df)) %>% 
  select(term, estimate, tau) %>% 
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) %>%
  select(tau, everything())

kable(rq_results)
```

### OLS

```{r}
ols = lm(t_mmr1 ~ p_change + h_hr + i_composite_score, data = cannabis_df)
#summary(ols)$coefficients

ols_results <- tidy(lm(t_mmr1 ~ p_change + h_hr + i_composite_score, data = cannabis_df)) %>% 
  select(term, estimate) %>% 
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) %>%
  select(everything())

kable(ols_results)
```

### Summary

Looking at the 3 summary tables for my implementation, rq(), and OLS, it looks
like my implementation (that used the simplex function to solve the LP problem)
and that of rq() generated the same results, but they different from the 
output obtained using linear regression. It looks like at quantiles 0.25 and 0.5,
quantile regression picks up on the positive associations of each of 
p_change, h_hr, and i_composite_score with t_mmr1, but at a quantile of 0.75,
the association of i_composite_score and t_mmr1 becomes negative, switching to -0.58.
In contrast, the mean obtained using linear regression has smaller beta values,
especially when we compare those with the results of quantile regression at the median (tau of 0.5).


## Problem 3: Implementation of LASSO


As illustrated in class, a LASSO problem can be rewritten as a quadratic programming problem.

1. Many widely used QP solvers require that the matrix in the quadratic function for the second
order term to be positive definite (such as `solve.QP` in the `quadprog` package). Rewrite the
quadratic programming problem for LASSO in matrix form and show that the matrix is not
positive definite, thus QP solvers like `solve.QP` cannot be used. 


From slide 27 of the lecture slides, we are trying to max:


$$
\max_{\beta_j^+, \beta_j^-} - \sum_i \left( y_i - \sum_j \beta_j^+ x_{ij} + \sum_j \beta_j^- x_{ij} \right)^2 \\
\text{s.t.} \sum_j \left( \beta_j^+ + \beta_j^- \right) \leq \lambda, \\
\beta_j^+, \beta_j^- \geq 0
$$

If we factor the $x_{ij}$ , we get 
$$
- \sum_i \left( y_i - \left [ x_{ij} (\sum_j (\beta_j^+  +\beta_j^-) \right ] \right)^2 
$$

Putting it into matrix form and redistributing, we get:

$$
(Y - \left [X(\beta_j^+ - \beta_j^-) \right ])^T(Y - \left [X(\beta_j^+ - \beta_j^-) \right ])
$$

$$
(Y - \left [X\beta_j^+ - X\beta_j^- \right ])^T(Y - \left [X\beta_j^+ - X\beta_j^- \right ])
$$

Defining new matrices:
$$
X' = 
\begin{bmatrix}
X \\
- X
\end{bmatrix}
$$

$$
\beta' = 
\begin{bmatrix}
\beta_j^+ \\
\beta_j^-
\end{bmatrix}
$$

we can rewrite as:

$$
(Y - X'\beta')^T(Y - X'\beta')
$$

Expanding, we can see that $\beta'^T X'^T X' \beta'$ is the second order term. Substituting back 
in our matrix definition of $X'$, we see that:

$$
\begin{aligned}
(Y - X'\beta')^T(Y - X'\beta') 
&= Y^T Y - 2 Y^T X' \beta' + \beta'^T X'^T X' \beta' \\
&= Y^T Y - 2 Y^T X' \beta' + \beta'^T 
\begin{bmatrix}
X^T X & -X^T X \\
- X^T X & X^T X
\end{bmatrix}
\beta'
\end{aligned}
$$



We can see that the determinant of this matrix is going to be 0 - by definition,
the determinant of a positive definite matrix is not 0, hence, this matrix is 
not positive definite.


2. The `LowRankQP` function in the `LowRankQP` package can handle the non positive definite situation. Use the
matrix format you derived above and `LowRankQP` to write your own function `my_lasso()` to
estimate the coefficients for a LASSO problem. Your function needs to take three parameters:
$Y$ (response), $X$ (predictor), and $lambda$ (tuning parameter), and return the estimated coefficients.


* Use your function to model `log(t_mmr1)` from the cannabis data using all other variables as potential covariates in the model
* Compare your results with those estimated using the `cv.glmnet` function in R from the `glmnet` package
* Summarize findings

The results will not be exactly the same because the estimation procedures are different, but trends (which variables are selected) should be similar.

```{r}
my_lasso <- function(y, X, lambda) {
  n <- length(y)
  p <- ncol(X)
  
  X_prime <- cbind(X, -X)
  
  Vmat <- t(X_prime) %*% X_prime
  dvec <- -t(X_prime) %*% y
  
  # We want sum of beta+ and beta- to be less than lambda
  Amat <- t(c(rep(1, p), rep(1, p)))
  bvec <- c(lambda)
  uvec <- c(rep(100, p), rep(100, p))
  
  
  fit <- LowRankQP(Vmat = Vmat,
                   dvec = dvec,
                   Amat = Amat,
                   bvec = bvec,
                   uvec = uvec) 
  
  # Extract beta = beta+ - beta-
  beta_plus  <- fit$alpha[1:p]
  beta_minus <- fit$alpha[(p + 1):(2 * p)]
  
  beta <- beta_plus - beta_minus
  
  return(beta)
  
}
```


```{r}
y <- log(cannabis_df$t_mmr1 + 0.0001) # adds small value to avoid log(0) otherwise I get an error
X <- cannabis_df %>%
  select(-id, -t_mmr1) %>%
  as.matrix()

lambda <- 0.5
my_lasso_results <- my_lasso(y, X, lambda)
```

```{r}
glmnet_fit <- cv.glmnet(X, y)
glmnet_results <- coef(glmnet_fit, s = "lambda.min")[-1]
glmnet_fit$lambda.min
```

```{r}
predictor_names <- colnames(X)
lasso_vec <- setNames(as.numeric(my_lasso_results), predictor_names)
glmnet_vec <- setNames(as.numeric(glmnet_results), predictor_names)

comparison_df <- bind_rows(
  lasso_vec,
  glmnet_vec
) %>%
  mutate(method = c("my_lasso", "glmnet")) %>%
  pivot_longer(
    cols = -method,
    names_to = "predictor",
    values_to = "coefficient"
  ) %>%
  pivot_wider(
    names_from = method,
    values_from = coefficient
  )

kable(comparison_df, digits = 4)
```



### Summary

After looking at the results of both my implementation of LASSO (with a lambda of 0.5) 
and those of cv.glmnet, the predictors selected were mostly the same, with both
models selecting p_fpc1, i_time_outside_reticle, and h_hr. Both models also
only selected a few sparse predictors. However, it is interesting
that i_prop_failed2 was selected in glmnet with a beta of -3.8, but not in my model,
and it would be interesting to further analyze why.


